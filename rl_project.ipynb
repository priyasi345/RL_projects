{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz8Jg9suMpxq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "886e8c4b-de95-4a98-e92e-d83ac67d6b36"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "env=gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "action_space_size=env.action_space.n\n",
        "state_space_size=env.observation_space.n\n",
        "\n",
        "q_table=np.zeros((state_space_size,action_space_size))\n",
        "print(q_table)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ClrVmkHN-gw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "d2650c1a-1084-469a-a822-2aee0e3c6c70"
      },
      "source": [
        "\n",
        "num_episodes=10000\n",
        "max_steps_per_episode=100\n",
        "learning_rate=0.1\n",
        "discount_rate=0.1\n",
        "discount_rate=0.99\n",
        "exploration_rate=1\n",
        "max_exploration_rate=1\n",
        "min_exploration_rate=0.01\n",
        "exploration_decay_rate=0.01\n",
        "\n",
        "rewards_all_episodes=[]\n",
        "#Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "  state=env.reset()\n",
        "  done=False\n",
        "  rewards_current_episode=0\n",
        "  for step in range(max_steps_per_episode):\n",
        "    #exploration-exploitation trade-off\n",
        "    exploration_rate_threshold=random.uniform(0,1)\n",
        "    if exploration_rate_threshold > exploration_rate:\n",
        "      action=np.argmax(q_table[state,:])\n",
        "    else:\n",
        "\n",
        "      action=env.action_space.sample()\n",
        "\n",
        "      new_state,reward,done,info=env.step(action)\n",
        "\n",
        "        #update Q-table for Q(s,a)\n",
        "  q_table[state,action]=q_table[state,action]*(1-learning_rate) + \\\n",
        "    learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
        "     \n",
        "  state=new_state\n",
        "  rewards_current_episode += rewards_current_episode\n",
        "  if done == True:\n",
        "        break\n",
        "        #Exploration rate decay\n",
        "        exploration_rate=min_exploration_rate+ \\\n",
        "        (max_exploration_rate-min_exploration_rate)* np.exp(-exploration_decay_rate*episodes)\n",
        "        rewards_all_episodes.append(rewards_current_episode)\n",
        "        #calculate and print tthe average reward per thousand episodes\n",
        "        rewards_per_thousand_episodes=np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
        "        count=1000\n",
        "        print(\"avg reward per thousand episodes\")\n",
        "        for r in rewards_per_thousand_episodes:\n",
        "          print(count,\":\",str(sum(r/1000)))\n",
        "          count += 1000\n",
        "          print ('q_table')\n",
        "          print(q_table)\n",
        "\n",
        "for episode in range(3):\n",
        "  state=env.reset()\n",
        "  done=False\n",
        "  print(\"episode\",episode+1,\"\\n\\n\\n\")\n",
        "  time.sleep(1)\n",
        "  for step in range(max_steps_per_episode):\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    time.sleep(0.3)\n",
        "    action=np.argmax(q_table[state,:])\n",
        "    new_state,reward,done,info=env.step(action)\n",
        "    if done:\n",
        "      clear_output(wait=True)\n",
        "      env.render()\n",
        "      if reward == 1:\n",
        "        print('goal')\n",
        "        time.sleep(3)\n",
        "      else:\n",
        "   \n",
        "          print('hole')\n",
        "          time.sleep(3)\n",
        "          clear_output(wait=True)\n",
        "          break\n",
        "          state=new_state\n",
        "          env.close()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "hole\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}